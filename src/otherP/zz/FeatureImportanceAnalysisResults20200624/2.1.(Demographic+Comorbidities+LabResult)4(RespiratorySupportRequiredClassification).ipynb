{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir='../data/Current Data/'\n",
    "data_dir = r\"C:\\work\\dev\\dECMT_src\\data_all\\COVID19_Data\\Current\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoricalFeatures=[]\n",
    "NumericFeatures=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_demographics=pd.read_csv(data_dir+'REACT_Demographics'+'.csv')\n",
    "#df_demographics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_demographics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoricalFeatures+=['GENDER','ETHNIC_GROUP', 'SMOKING_HISTORY']\n",
    "NumericFeatures+=['PATIENT_AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyid_data_dic=dict()\n",
    "for studyid,age,gender,ethnic,smoking in df_demographics[['STUDY_ID','PATIENT_AGE','GENDER','ETHNIC_GROUP', 'SMOKING_HISTORY']].values:\n",
    "    if age is not np.nan and gender is not np.nan and ethnic is not np.nan and smoking is not np.nan:\n",
    "        if studyid not in studyid_data_dic:\n",
    "            studyid_data_dic[studyid]=dict()\n",
    "        studyid_data_dic[studyid]['PATIENT_AGE']=age\n",
    "        studyid_data_dic[studyid]['GENDER']=str(gender)\n",
    "        studyid_data_dic[studyid]['ETHNIC_GROUP']=ethnic\n",
    "        studyid_data_dic[studyid]['SMOKING_HISTORY']=smoking\n",
    "#studyid_data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comorbidities=pd.read_csv(data_dir+'REACT_Comorbidities_unpivoted'+'.csv')\n",
    "#df_demographics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comorbidities=list(df_comorbidities.COMORBIDITY.unique())\n",
    "NumericFeatures+=comorbidities\n",
    "for studyid in studyid_data_dic:\n",
    "    for co in comorbidities:\n",
    "        studyid_data_dic[studyid][co]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for studyid,co,status in df_comorbidities.values:\n",
    "    if studyid in studyid_data_dic:\n",
    "        studyid_data_dic[studyid][co]=status\n",
    "#studyid_data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label: RespiratorySupportRequired or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Vitalsigns_Categorical=pd.read_csv(data_dir+'REACT_Vitalsigns_Categorical'+'.csv')\n",
    "#df_Vitalsigns_Categorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Vitalsigns_Categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'Air - Not Supported': 0,\n 'Trachy Mask': 1,\n 'NIV - BIPAP full face mask': 1,\n 'Venturi Mask': 1,\n 'Face Mask': 1,\n 'Invasive Ventilation': 1,\n 'NIV - CPAP face mask': 1,\n 'NIV - BIPAP nasal mask': 1,\n 'NIV - CPAP nasal mask': 1,\n 'Optiflow / Hi Flow': 1,\n 'Nasal Specs': 1,\n 'Non-Rebreath Mask': 1,\n 'NIV - BIPAP face mask': 1,\n 'NIV - CPAP full face mask': 1}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CategorySet=dict()\n",
    "CategorySet[0]=set(['Air - Not Supported'])\n",
    "CategorySet[1]=set([\n",
    "    'Nasal Specs', 'Face Mask', 'Venturi Mask',\n",
    "    'Non-Rebreath Mask', 'Invasive Ventilation', 'Optiflow / Hi Flow',\n",
    "    'NIV - CPAP face mask', 'NIV - CPAP full face mask',\n",
    "    'NIV - BIPAP face mask', 'NIV - CPAP nasal mask',\n",
    "    'NIV - BIPAP nasal mask', 'NIV - BIPAP full face mask',\n",
    "    'Trachy Mask'\n",
    "])\n",
    "CategoryDictionary=dict([(item,label) for label in CategorySet for item in CategorySet[label]])\n",
    "CategoryDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label=df_Vitalsigns_Categorical[df_Vitalsigns_Categorical.PARAMETER=='Respiratory Support'][['STUDY_ID','UNITFROM_DATETIME','VALUE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "label_studyid_time_dic=dict()\n",
    "for study_id,start_datetime,label in df_label.values:\n",
    "    start_datetime=start_datetime.replace('.000','')#need to be fixed some datetime end with .000\n",
    "    start_datetime=datetime.strptime(start_datetime,'%Y-%m-%d %H:%M:%S')\n",
    "    label_category=CategoryDictionary[label]\n",
    "    if study_id not in label_studyid_time_dic:\n",
    "        label_studyid_time_dic[study_id]=[start_datetime,label_category]\n",
    "    else:\n",
    "        curr_start_datetime,curr_label_category=label_studyid_time_dic[study_id]\n",
    "        if label_category>=curr_label_category and curr_start_datetime>start_datetime:\n",
    "            label_studyid_time_dic[study_id]=[start_datetime,label_category]\n",
    "\n",
    "#label_studyid_time_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for studyid in studyid_data_dic:\n",
    "    if studyid not in label_studyid_time_dic:\n",
    "        studyid_data_dic[studyid]['TIME_POINT']=None\n",
    "        studyid_data_dic[studyid]['PREDICTION_LABEL']=0\n",
    "    else:\n",
    "        start_datetime,label_category=label_studyid_time_dic[studyid]\n",
    "        studyid_data_dic[studyid]['TIME_POINT']=start_datetime\n",
    "        studyid_data_dic[studyid]['PREDICTION_LABEL']=label_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labresults=pd.read_csv(data_dir+'REACT_LabResults'+'.csv')\n",
    "#df_labresults.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_labresults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcode_lst=list(df_labresults.REACT_TESTCODE.unique())\n",
    "testcode2id=dict([(testcode_lst[i],i) for i in range(len(testcode_lst))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "studyid_testcode_time_numeric_dic=dict()\n",
    "for studyid,test_time,testcode,numeric,lower,upper in df_labresults[[\n",
    "    'STUDY_ID','PATHOLOGY_SPECIMEN_DATE','REACT_TESTCODE',\n",
    "    'PATHOLOGY_RESULT_NUMERIC','LOWER_RANGE','UPPER_RANGE']].values:\n",
    "    test_time=datetime.strptime(test_time,'%Y-%m-%d %H:%M:%S')\n",
    "    if studyid not in studyid_testcode_time_numeric_dic:\n",
    "        studyid_testcode_time_numeric_dic[studyid]=[]\n",
    "    studyid_testcode_time_numeric_dic[studyid].append([test_time,testcode,numeric])\n",
    "#studyid_testcode_time_numeric_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumericFeatures+=[testcode+suffix for testcode in testcode_lst for suffix in ['_MIN','_MAX','_MEAN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for studyid in studyid_data_dic:\n",
    "    for testcode in testcode_lst:\n",
    "        for suffix in ['_MIN','_MAX','_MEAN']:\n",
    "            studyid_data_dic[studyid][testcode+suffix]=None\n",
    "    time_point=studyid_data_dic[studyid]['TIME_POINT']\n",
    "    if studyid in studyid_testcode_time_numeric_dic:\n",
    "        individual_labresult_dic=dict()\n",
    "        for test_time,testcode,numeric in studyid_testcode_time_numeric_dic[studyid]:\n",
    "            if time_point is None or test_time<time_point:\n",
    "                if testcode not in individual_labresult_dic:\n",
    "                    individual_labresult_dic[testcode]=[]\n",
    "                individual_labresult_dic[testcode].append(numeric)\n",
    "        for testcode in individual_labresult_dic:\n",
    "            studyid_data_dic[studyid][testcode+'_MIN']=np.min(individual_labresult_dic[testcode])\n",
    "            studyid_data_dic[studyid][testcode+'_MAX']=np.min(individual_labresult_dic[testcode])\n",
    "            studyid_data_dic[studyid][testcode+'_MEAN']=np.mean(individual_labresult_dic[testcode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyid_lst=list(studyid_data_dic.keys())\n",
    "#studyid_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_categorical=[[studyid_data_dic[studyid][k] for k in CategoricalFeatures] for studyid in studyid_lst]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_onehot=enc.fit_transform(X_categorical).toarray()\n",
    "\n",
    "X_cols+=list(enc.get_feature_names(CategoricalFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric=[[studyid_data_dic[studyid][k] for k in NumericFeatures] for studyid in studyid_lst]\n",
    "X_cols+=NumericFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing_value=np.concatenate((X_onehot,X_numeric), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array([studyid_data_dic[studyid]['PREDICTION_LABEL'] for studyid in studyid_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.826865671641791"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_ratio=len([_ for v in Y if v==0])/len([_ for v in Y if v==1])\n",
    "balance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from functools import reduce\n",
    "\n",
    "XY_missing_value_balanced=np.array(\n",
    "    reduce(lambda a,b:a+b,[[list(x)+[y]]*(balance_ratio if y==1 else 1) for x,y in zip(X_missing_value,Y)])\n",
    ")\n",
    "#XY_missing_value_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill in nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X=imputer.fit_transform(X_missing_value[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Classification task (RespiratorySupportRequired:1 or not:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models={\n",
    "    \"Logit\":LogisticRegression(solver=\"lbfgs\"),\n",
    "    \"MLP\":MLPClassifier(max_iter=3000,solver='adam', alpha=1e-3,hidden_layer_sizes=(64)),\n",
    "    \"DecisionTree\":DecisionTreeClassifier(),\n",
    "    \"RandomForest\":RandomForestClassifier(n_estimators=100),\n",
    "    \"AdaBoost\":AdaBoostClassifier(n_estimators=100),\n",
    "    \"GradientBoost\":GradientBoostingClassifier(n_estimators=100),\n",
    "    \"XGBoost\":XGBClassifier(n_estimators=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Logit       MLP  DecisionTree  RandomForest  AdaBoost  GradientBoost  \\\n",
      "0  0.555315  0.550588      0.472015      0.552513  0.505853       0.551033   \n",
      "1  0.524854  0.590504      0.547829      0.521746  0.471443       0.574415   \n",
      "2  0.650982  0.617922      0.525199      0.623482  0.570411       0.584342   \n",
      "3  0.519934  0.480721      0.480639      0.629975  0.526477       0.604769   \n",
      "4  0.602486  0.537256      0.513567      0.508478  0.504359       0.585612   \n",
      "5  0.608466  0.619951      0.525754      0.502282  0.538252       0.528892   \n",
      "6  0.543017  0.540695      0.621949      0.654360  0.611511       0.650599   \n",
      "7  0.708817  0.627542      0.512391      0.575694  0.585472       0.607442   \n",
      "8  0.499619  0.548208      0.548005      0.622950  0.559462       0.596080   \n",
      "9  0.588180  0.537157      0.506618      0.550700  0.600559       0.616327   \n",
      "\n",
      "    XGBoost  \n",
      "0  0.533504  \n",
      "1  0.577811  \n",
      "2  0.603359  \n",
      "3  0.582111  \n",
      "4  0.560394  \n",
      "5  0.573822  \n",
      "6  0.669118  \n",
      "7  0.643567  \n",
      "8  0.612696  \n",
      "9  0.599864  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_result_lst=[]\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "n_class=2\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    Y_test = np.zeros([len(y_test),n_class])\n",
    "    for i in range(len(y_test)):\n",
    "        Y_test[i,y_test[i]]=1\n",
    "    ap_result_lst.append(\n",
    "        [average_precision_score(Y_test,models[model_name].fit(X_train,y_train).predict_proba(X_test))\\\n",
    "                for model_name in models])\n",
    "\n",
    "print(pd.DataFrame(ap_result_lst,columns=[model_name for model_name in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Logit       MLP  DecisionTree  RandomForest  AdaBoost  GradientBoost  \\\n",
      "0  0.580167  0.565054      0.525397      0.574218   0.54738       0.589951   \n",
      "\n",
      "    XGBoost  \n",
      "0  0.595625  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(np.mean(ap_result_lst,axis=0,keepdims=True),columns=[model_name for model_name in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['Not required','Required']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver=\"lbfgs\").fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-37-c2c50626212d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mshap\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# explain the model's predictions using SHAP\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mexplainer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mshap\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLinearExplainer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclf\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "explainer = shap.LinearExplainer(clf,X)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X, feature_names=X_cols, plot_type=\"bar\",class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_cols,clf.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "for k,v in sorted(\n",
    "    [(X_cols[i],feature_importance_dic[X_cols[i]]) for i in SelectFromModel(clf, prefit=True, max_features=20).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    "):\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X, feature_names=X_cols, plot_type=\"bar\",class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_cols,clf.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "for k,v in sorted(\n",
    "    [(X_cols[i],feature_importance_dic[X_cols[i]]) for i in SelectFromModel(clf, prefit=True, max_features=20).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    "):\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X, feature_names=X_cols, plot_type=\"bar\",class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(n_estimators=100).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_cols,clf.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "for k,v in sorted(\n",
    "    [(X_cols[i],feature_importance_dic[X_cols[i]]) for i in SelectFromModel(clf, prefit=True, max_features=20).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    "):\n",
    "    print(k,':',v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}