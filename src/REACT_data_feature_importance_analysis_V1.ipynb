{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zili\n",
    "import csv\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data/'\n",
    "\n",
    "#table='REACT_COVID_Demographics'\n",
    "#table='REACT_COVID_Demographics_20200506'\n",
    "#table='REACT_Events'\n",
    "#table='REACT_LabResults'\n",
    "#table='REACT_PharmacyData'\n",
    "#table='REACT_UHSCOVIDTest_processed'\n",
    "#table='REACT_Vitalsigns_Categorical'\n",
    "#table='REACT_Vitalsigns_Numeric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df=pd.read_csv(data_dir+table+'.csv')\n",
    "#df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract AIR_ventilator information from table REACT_Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'Air - Not Supported': 0.0,\n 'Nasal Specs': 1.0,\n 'Face Mask': 2.0,\n 'Venturi Mask': 3.0,\n 'Trachy Mask': 4.0,\n 'Non-Rebreath Mask': 5.0,\n 'Optiflow / Hi Flow': 6.0,\n 'NIV - CPAP nasal mask': 7.0,\n 'NIV - CPAP face mask': 8.0,\n 'NIV - CPAP full face mask': 9.0,\n 'NIV - BIPAP nasal mask': 10.0,\n 'NIV - BIPAP face mask': 11.0,\n 'NIV - BIPAP full face mask': 12.0,\n 'Invasive Ventilation': 13.0}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_ventilator_degree=[\n",
    "    'Air - Not Supported',\n",
    "    'Nasal Specs',\n",
    "    'Face Mask',\n",
    "    'Venturi Mask',\n",
    "    'Trachy Mask',\n",
    "    'Non-Rebreath Mask',\n",
    "    'Optiflow / Hi Flow',\n",
    "    'NIV - CPAP nasal mask',\n",
    "    'NIV - CPAP face mask',\n",
    "    'NIV - CPAP full face mask',\n",
    "    'NIV - BIPAP nasal mask',\n",
    "    'NIV - BIPAP face mask',\n",
    "    'NIV - BIPAP full face mask',\n",
    "    'Invasive Ventilation'\n",
    "]\n",
    "air_ventilator_degree_dic=dict([(air_ventilator_degree[i],float(i)) for i in range(len(air_ventilator_degree))])\n",
    "air_ventilator_degree_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/REACT_Vitalsigns_Categorical.csv does not exist: 'data/REACT_Vitalsigns_Categorical.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-08284b3247a3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf_REACT_Vitalsigns_Categorical\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_dir\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m'REACT_Vitalsigns_Categorical'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m'.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;31m#df_REACT_Vitalsigns_Categorical.info()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dECMT\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[0;32m    674\u001B[0m         )\n\u001B[0;32m    675\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 676\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    677\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    678\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dECMT\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    446\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    447\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 448\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    449\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    450\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dECMT\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    878\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    879\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 880\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    881\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    882\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dECMT\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1112\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"c\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1113\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"c\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1114\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1115\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1116\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"python\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dECMT\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1889\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"usecols\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1890\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1891\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1892\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1893\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] File data/REACT_Vitalsigns_Categorical.csv does not exist: 'data/REACT_Vitalsigns_Categorical.csv'"
     ]
    }
   ],
   "source": [
    "df_REACT_Vitalsigns_Categorical=pd.read_csv(data_dir+'REACT_Vitalsigns_Categorical'+'.csv')\n",
    "#df_REACT_Vitalsigns_Categorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyid_air_ventilator_dic=dict()\n",
    "\n",
    "for studyid,air_vent in df_REACT_Vitalsigns_Categorical[df_REACT_Vitalsigns_Categorical.VALUE.isin(air_ventilator_degree_dic)][['STUDY_ID','VALUE']].values:\n",
    "    if studyid not in studyid_air_ventilator_dic or studyid_air_ventilator_dic[studyid]<air_ventilator_degree_dic[air_vent]:\n",
    "        studyid_air_ventilator_dic[studyid]=air_ventilator_degree_dic[air_vent]\n",
    "\n",
    "df_studyid_air_vent=pd.DataFrame(\n",
    "    data=[[key,studyid_air_ventilator_dic[key]] for key in studyid_air_ventilator_dic],\n",
    "    columns=['STUDY_ID','AIR_VENT_DEGREE']\n",
    ")\n",
    "#df_studyid_air_vent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract ITU_DAYS information from table REACT_Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REACT_Events=pd.read_csv(data_dir+'REACT_Events'+'.csv')\n",
    "#df_REACT_Events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def str2date(ss):\n",
    "    day,month,year=ss.split('/')\n",
    "    return date(int(year),int(month),int(day))\n",
    "\n",
    "max_date=max([str2date(x) for x in df_REACT_Events[df_REACT_Events.END_DATE.notnull()].END_DATE.unique()])\n",
    "max_date=str(max_date.day)+'/'+str(max_date.month)+'/'+str(max_date.year)\n",
    "\n",
    "#max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "studyid_ITU_lst=[]\n",
    "for studyid,start_date,end_date in df_REACT_Events[df_REACT_Events.EVENT_TYPE=='ITU'][['STUDY_ID','START_DATE','END_DATE']].fillna({'END_DATE':max_date}).values:\n",
    "    start_day,start_month,start_year=map(int,start_date.split('/'))\n",
    "    end_day,end_month,end_year=map(int,end_date.split('/'))\n",
    "    days=int((date(end_year,end_month,end_day)-date(start_year,start_month,start_day)).days)\n",
    "    studyid_ITU_lst.append([studyid,days])\n",
    "df_studyid_ITU=pd.DataFrame(data=studyid_ITU_lst,columns=['STUDY_ID','ITU_DAYS'])\n",
    "#df_studyid_ITU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load table REACT\\_COVID\\_Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics=pd.read_csv(data_dir+'REACT_COVID_Demographics'+'.csv')\n",
    "#df_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_demographics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_demographics.STUDY_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_demographics[['STUDY_ID','PATIENT_AGE','GENDER','ETHNIC_GROUP','IS_PREGNANT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load table REACT_LabResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labresults=pd.read_csv(data_dir+'REACT_LabResults'+'.csv')\n",
    "#df_labresults.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_labresults.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_labresults.STUDY_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcode_lst=df_labresults.REACT_TESTCODE.unique()\n",
    "testcode2id=dict([(testcode_lst[i],i) for i in range(len(testcode_lst))])\n",
    "#testcode_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyid_textcode_dic=dict()\n",
    "for studyid,testcode,result in df_labresults[['STUDY_ID','REACT_TESTCODE','PATHOLOGY_RESULT_NUMERIC']].values:\n",
    "    if studyid not in studyid_textcode_dic:\n",
    "        studyid_textcode_dic[studyid]=dict()\n",
    "    if testcode not in studyid_textcode_dic[studyid]:\n",
    "        studyid_textcode_dic[studyid][testcode]=[]\n",
    "    studyid_textcode_dic[studyid][testcode].append(result)\n",
    "#studyid_textcode_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labresults_dic=dict()\n",
    "for studyid in studyid_textcode_dic:\n",
    "    if studyid not in labresults_dic:\n",
    "        labresults_dic[studyid]=[None]*len(testcode_lst)*3\n",
    "    for testcode in studyid_textcode_dic[studyid]:\n",
    "        results=studyid_textcode_dic[studyid][testcode]\n",
    "        labresults_dic[studyid][testcode2id[testcode]*3:(testcode2id[testcode]+1)*3]=[np.min(results),np.max(results),np.mean(results)]\n",
    "#labresults_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studyid_testcode_result=pd.DataFrame(\n",
    "    data=[[studyid]+labresults_dic[studyid] for studyid in labresults_dic],\n",
    "    columns=[\"STUDY_ID\"]+[testcode+suffix for testcode in testcode_lst for suffix in ['_min','_max','_mean']]\n",
    ")\n",
    "#df_studyid_testcode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_studyid_testcode_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_features=df_demographics[['STUDY_ID','PATIENT_AGE','GENDER','ETHNIC_GROUP','IS_PREGNANT']].merge(\n",
    "    df_studyid_testcode_result,how='inner',on='STUDY_ID'\n",
    ")\n",
    "#df_patient_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_patient_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataframes, fill in default value (0.0) for $\\textbf{NaN}$ value in columns ITU_DAYS and AIR_VENT_DEGREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_features_labels=df_patient_features.merge(\n",
    "    df_studyid_ITU,how='left',on='STUDY_ID'\n",
    ").fillna({'ITU_DAYS':0}).merge(\n",
    "    df_studyid_air_vent,how='left',on='STUDY_ID'\n",
    ").fillna({'AIR_VENT_DEGREE':0.0})\n",
    "#df_patient_features_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_patient_features_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_features_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X Feature Preprocessing, OneHotEncoding for Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature_name_lst=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_categorical=df_patient_features_labels[['GENDER', 'ETHNIC_GROUP', 'IS_PREGNANT']]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_onehot=enc.fit_transform(X_categorical).toarray()\n",
    "\n",
    "X_feature_name_lst+=list(enc.get_feature_names(['GENDER', 'ETHNIC_GROUP', 'IS_PREGNANT']))\n",
    "#X_feature_name_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_features_lst=[\n",
    "    'PATIENT_AGE',\n",
    "    'POTASSIUM_min', 'POTASSIUM_max', 'POTASSIUM_mean', 'ALT_min',\n",
    "    'ALT_max', 'ALT_mean', 'BILIRUBIN_min', 'BILIRUBIN_max',\n",
    "    'BILIRUBIN_mean', 'UREA_min', 'UREA_max', 'UREA_mean', 'CREATENINE_min',\n",
    "    'CREATENINE_max', 'CREATENINE_mean', 'SODIUM_min', 'SODIUM_max',\n",
    "    'SODIUM_mean', 'CRP_min', 'CRP_max', 'CRP_mean', 'NEUTROPHILS_min',\n",
    "    'NEUTROPHILS_max', 'NEUTROPHILS_mean', 'EOSINOPHILS_min',\n",
    "    'EOSINOPHILS_max', 'EOSINOPHILS_mean', 'HB_min', 'HB_max', 'HB_mean',\n",
    "    'LYMPHOCYTES_min', 'LYMPHOCYTES_max', 'LYMPHOCYTES_mean', 'WBC_min',\n",
    "    'WBC_max', 'WBC_mean', 'PLATELETS_min', 'PLATELETS_max',\n",
    "    'PLATELETS_mean', 'GLUCOSE_min', 'GLUCOSE_max', 'GLUCOSE_mean',\n",
    "    'TRIGYCERIN_min', 'TRIGYCERIN_max', 'TRIGYCERIN_mean', 'FERRITIN_min',\n",
    "    'FERRITIN_max', 'FERRITIN_mean', 'AST_min', 'AST_max', 'AST_mean',\n",
    "    'TROPONIN_min', 'TROPONIN_max', 'TROPONIN_mean', 'LDH_min', 'LDH_max',\n",
    "    'LDH_mean', 'D_DIMER_min', 'D_DIMER_max', 'D_DIMER_mean'\n",
    "]\n",
    "\n",
    "X_feature_name_lst+=numeric_features_lst\n",
    "\n",
    "#X_feature_name_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric=df_patient_features_labels[numeric_features_lst].values\n",
    "#X_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing=np.concatenate([X_onehot, X_numeric], axis=1)\n",
    "#X_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill in missing features value with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.impute import KNNImputer\n",
    "\n",
    "#imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X=imputer.fit_transform(X_missing)\n",
    "\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Y labels ITU_DAYS, AIR_VENTILATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ITU=df_patient_features_labels.values[:,-2]\n",
    "Y_AIR=df_patient_features_labels.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITU_DAYS prediction test, evaluated by R2_score:\n",
    "$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "models={\n",
    "    \"Linear\":LinearRegression(),\n",
    "    \"MLP\":MLPRegressor(max_iter=3000,solver='adam', alpha=1e-3,hidden_layer_sizes=(64, 16)),\n",
    "    \"DecisionTree\":DecisionTreeRegressor(),\n",
    "    \"RandomForest\":RandomForestRegressor(),\n",
    "    \"AdaBoost\":AdaBoostRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\":GradientBoostingRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_result_lst=[]\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y_ITU[train_index], Y_ITU[test_index]\n",
    "    r2_result_lst.append(\n",
    "        [r2_score(y_test,models[model_name].fit(X_train,y_train).predict(X_test))\n",
    "         for model_name in models])\n",
    "\n",
    "print(pd.DataFrame(r2_result_lst,columns=[model_name for model_name in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITU_DAYS feature importance analysis $\\textbf{RandomForest}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor().fit(X, Y_ITU)\n",
    "\n",
    "for feature,importance in zip(X_feature_name_lst,regr.feature_importances_):\n",
    "    print(feature,\":\",importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_feature_name_lst,regr.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sorted(\n",
    "    [(X_feature_name_lst[i],feature_importance_dic[X_feature_name_lst[i]]) for i in SelectFromModel(regr, prefit=True, max_features=10).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITU_DAYS feature importance analysis $\\textbf{AdaBoost}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "regr = AdaBoostRegressor().fit(X, Y_ITU)\n",
    "\n",
    "for feature,importance in zip(X_feature_name_lst,regr.feature_importances_):\n",
    "    print(feature,\":\",importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_feature_name_lst,regr.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sorted(\n",
    "    [(X_feature_name_lst[i],feature_importance_dic[X_feature_name_lst[i]]) for i in SelectFromModel(regr, prefit=True, max_features=10).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITU_DAYS feature importance analysis $\\textbf{GradientBoostingTree}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regr = GradientBoostingRegressor().fit(X, Y_ITU)\n",
    "\n",
    "for feature,importance in zip(X_feature_name_lst,regr.feature_importances_):\n",
    "    print(feature,\":\",importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_feature_name_lst,regr.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sorted(\n",
    "    [(X_feature_name_lst[i],feature_importance_dic[X_feature_name_lst[i]]) for i in SelectFromModel(regr, prefit=True, max_features=10).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIR_ventilator prediction test, evaluated by R2_score:\n",
    "$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "models={\n",
    "    \"Linear\":LinearRegression(),\n",
    "    \"MLP\":MLPRegressor(max_iter=3000,solver='adam', alpha=1e-3,hidden_layer_sizes=(64, 16)),\n",
    "    \"DecisionTree\":DecisionTreeRegressor(),\n",
    "    \"RandomForest\":RandomForestRegressor(),\n",
    "    \"AdaBoost\":AdaBoostRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\":GradientBoostingRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_result_lst=[]\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y_AIR[train_index], Y_AIR[test_index]\n",
    "    r2_result_lst.append(\n",
    "        [r2_score(y_test,models[model_name].fit(X_train,y_train).predict(X_test))\n",
    "         for model_name in models])\n",
    "\n",
    "print(pd.DataFrame(r2_result_lst,columns=[model_name for model_name in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIR_ventilator feature importance analysis $\\textbf{RandomForest}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor().fit(X, Y_AIR)\n",
    "\n",
    "for feature,importance in zip(X_feature_name_lst,regr.feature_importances_):\n",
    "    print(feature,\":\",importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_feature_name_lst,regr.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sorted(\n",
    "    [(X_feature_name_lst[i],feature_importance_dic[X_feature_name_lst[i]]) for i in SelectFromModel(regr, prefit=True, max_features=10).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIR_ventilator feature importance analysis $\\textbf{AdaBoost}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "regr = AdaBoostRegressor().fit(X, Y_AIR)\n",
    "\n",
    "for feature,importance in zip(X_feature_name_lst,regr.feature_importances_):\n",
    "    print(feature,\":\",importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_feature_name_lst,regr.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sorted(\n",
    "    [(X_feature_name_lst[i],feature_importance_dic[X_feature_name_lst[i]]) for i in SelectFromModel(regr, prefit=True, max_features=10).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIR_ventilator feature importance analysis $\\textbf{GradientBoostingTree}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regr = GradientBoostingRegressor().fit(X, Y_AIR)\n",
    "\n",
    "for feature,importance in zip(X_feature_name_lst,regr.feature_importances_):\n",
    "    print(feature,\":\",importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_dic=dict(zip(X_feature_name_lst,regr.feature_importances_))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sorted(\n",
    "    [(X_feature_name_lst[i],feature_importance_dic[X_feature_name_lst[i]]) for i in SelectFromModel(regr, prefit=True, max_features=10).get_support(indices=True)],\n",
    "    key=lambda x:x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}